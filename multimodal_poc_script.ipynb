{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "763204d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from IPython.display import Image, Audio, display\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import pipeline\n",
    "import soundfile as sf\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema import OutputParserException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9923a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"poc_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3679ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74414/2832011667.py:26: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3\")\n",
      "/tmp/ipykernel_74414/2832011667.py:27: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "class Triple(BaseModel):\n",
    "    head: str\n",
    "    relation: str\n",
    "    tail: str\n",
    "\n",
    "class TripleOutput(BaseModel):\n",
    "    triples: List[Triple]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=TripleOutput)\n",
    "\n",
    "TEMPLATE = \"\"\"You are an information extraction system.\n",
    "Extract all (head_entity, relation, tail_entity) triples from the text below.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Text: {input_text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=TEMPLATE,\n",
    "    input_variables=[\"input_text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def extract_triples(text: str, retries: int = 2) -> TripleOutput:\n",
    "    for attempt in range(retries + 1):\n",
    "        raw_output = chain.run({\"input_text\": text})\n",
    "        try:\n",
    "            return parser.parse(raw_output)\n",
    "        except OutputParserException as e:\n",
    "            print(f\"Parse failed ({attempt+1}/{retries+1}): {e}\")\n",
    "            if attempt == retries:\n",
    "                return TripleOutput(triples=[])\n",
    "    return TripleOutput(triples=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229de177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_triples(parsed: TripleOutput):\n",
    "    G = nx.DiGraph()\n",
    "    for t in parsed.triples:\n",
    "        G.add_node(t.head, type=\"Entity\")\n",
    "        G.add_node(t.tail, type=\"Entity\")\n",
    "        G.add_edge(t.head, t.tail, relation=t.relation)\n",
    "    return G\n",
    "\n",
    "def draw_graph(G, outpath):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=900, node_color=\"skyblue\")\n",
    "    nx.draw_networkx_labels(G, pos, font_size=9)\n",
    "    edge_labels = {(u,v): d.get(\"relation\",\"\") for u,v,d in G.edges(data=True)}\n",
    "    nx.draw_networkx_edges(G, pos, arrows=True)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=150)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ae6ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_modalities(G):\n",
    "    sound_relations = {\"sounds like\", \"emits\", \"produces\", \"plays\", \"sings\"}\n",
    "    visual_relations = {\"occurs over\", \"is located on\", \"appears in\", \"shows\", \"illustrates\"}\n",
    "\n",
    "    want_audio, want_image = False, False\n",
    "    audio_prompts, image_prompts = [], []\n",
    "\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        rel = data.get(\"relation\", \"\").lower().strip()\n",
    "        if rel in sound_relations or \"sound\" in rel:\n",
    "            want_audio = True\n",
    "            audio_prompts.append(f\"{u} {rel} {v}\")\n",
    "        if rel in visual_relations:\n",
    "            want_image = True\n",
    "            image_prompts.append(f\"{u} {rel} {v}\")\n",
    "\n",
    "    entity_text = \" \".join(G.nodes()).lower()\n",
    "    if any(word in entity_text for word in [\"storm\",\"sky\",\"forest\",\"ocean\",\"meadow\",\"lighthouse\",\"city\",\"coast\"]):\n",
    "        want_image = True\n",
    "        image_prompts.append(entity_text)\n",
    "\n",
    "    return {\"image\": want_image, \"audio\": want_audio,\n",
    "            \"image_prompts\": image_prompts, \"audio_prompts\": audio_prompts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f1182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_diffusion(pipe_model=\"stabilityai/sd-turbo\", device=\"auto\"):\n",
    "    if device == \"cpu\":\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            pipe_model,\n",
    "            torch_dtype=torch.float32 \n",
    "        )\n",
    "        pipe = pipe.to(\"cpu\")\n",
    "    else:\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            pipe_model,\n",
    "            torch_dtype=torch.float16 \n",
    "        )\n",
    "        pipe = pipe.to(\"CUDA\")\n",
    "        pipe.enable_attention_slicing()\n",
    "    return pipe\n",
    "\n",
    "def generate_image(pipe, prompt, outpath):\n",
    "    seed = int(hashlib.sha1(prompt.encode()).hexdigest()[:8], 16) % (2**31)\n",
    "    generator = torch.Generator(device=DEVICE).manual_seed(seed)\n",
    "    image = pipe(prompt, height=256, width=256, num_inference_steps=10).images[0]\n",
    "\n",
    "    image.save(outpath)\n",
    "    return outpath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d30b7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import soundfile as sf\n",
    "\n",
    "musicgen = pipeline(\n",
    "    \"text-to-audio\",\n",
    "    model=\"facebook/musicgen-small\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_audio(prompt: str, out_wav: str):\n",
    "    audio = musicgen(prompt, max_new_tokens=256)\n",
    "    audio_array = audio[0][\"array\"]\n",
    "    sampling_rate = audio[0][\"sampling_rate\"]\n",
    "    sf.write(out_wav, audio_array, sampling_rate)\n",
    "    return out_wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d3c4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_poc(user_prompt):\n",
    "    print(\"Extracting triples with Llama3...\")\n",
    "    triples = extract_triples(user_prompt)\n",
    "    print(\"Extracted triples:\", triples)\n",
    "\n",
    "    G = build_graph_from_triples(triples)\n",
    "    graph_path = os.path.join(OUTPUT_DIR, \"kg_graph.png\")\n",
    "    draw_graph(G, graph_path)\n",
    "\n",
    "    decisions = decide_modalities(G)\n",
    "    print(\"Decisions:\", decisions)\n",
    "\n",
    "    results = {\"graph\": graph_path, \"json\": triples.model_dump(), \"outputs\": {}}\n",
    "\n",
    "    if decisions[\"image\"]:\n",
    "        print(\"Generating image...\")\n",
    "        pipe = init_diffusion()\n",
    "        img_path = os.path.join(OUTPUT_DIR, \"generated.png\")\n",
    "        img_prompt = \", \".join(decisions[\"image_prompts\"]) or user_prompt\n",
    "        generate_image(pipe, img_prompt, img_path)\n",
    "        results[\"outputs\"][\"image\"] = img_path\n",
    "\n",
    "    if decisions[\"audio\"]:\n",
    "        print(\"Generating audio...\")\n",
    "        wav_path = os.path.join(OUTPUT_DIR, \"generated.wav\")\n",
    "        audio_prompt = \", \".join(decisions[\"audio_prompts\"]) or user_prompt\n",
    "        generate_audio(audio_prompt, wav_path)\n",
    "        results[\"outputs\"][\"audio\"] = wav_path\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d2d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74414/2832011667.py:31: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  raw_output = chain.run({\"input_text\": text})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting triples with Llama3...\n",
      "Extracted triples: triples=[Triple(head='Show me a dramatic thunderstorm', relation='describes', tail='over a lighthouse'), Triple(head='how thunder sounds', relation='explains', tail='thunderstorms')]\n",
      "Decisions: {'image': True, 'audio': False, 'image_prompts': ['show me a dramatic thunderstorm over a lighthouse how thunder sounds thunderstorms'], 'audio_prompts': []}\n",
      "Generating image...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce7ea5c30ac4ae6af1146c342597bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, maia, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: DEVICE",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     prompt = \u001b[33m\"\u001b[39m\u001b[33mShow me a dramatic thunderstorm over a lighthouse and also provide how thunder sounds.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     out = \u001b[43mmultimodal_poc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDone. Results:\u001b[39m\u001b[33m\"\u001b[39m, out)\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# If inside Jupyter\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mmultimodal_poc\u001b[39m\u001b[34m(user_prompt)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decisions[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating image...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     pipe = \u001b[43minit_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     img_path = os.path.join(OUTPUT_DIR, \u001b[33m\"\u001b[39m\u001b[33mgenerated.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     img_prompt = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(decisions[\u001b[33m\"\u001b[39m\u001b[33mimage_prompts\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;129;01mor\u001b[39;00m user_prompt\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36minit_diffusion\u001b[39m\u001b[34m(pipe_model, device)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      9\u001b[39m     pipe = StableDiffusionPipeline.from_pretrained(\n\u001b[32m     10\u001b[39m         pipe_model,\n\u001b[32m     11\u001b[39m         torch_dtype=torch.float16 \n\u001b[32m     12\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     pipe = \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDEVICE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     pipe.enable_attention_slicing()\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pipe\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/multimodal/.venv/lib/python3.12/site-packages/diffusers/pipelines/pipeline_utils.py:415\u001b[39m, in \u001b[36mDiffusionPipeline.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    413\u001b[39m         dtype_arg = args[\u001b[32m0\u001b[39m]\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m         device_arg = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) == \u001b[32m2\u001b[39m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[32m0\u001b[39m], torch.dtype):\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, maia, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: DEVICE"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Show me a dramatic thunderstorm over a lighthouse and also provide how thunder sounds.\"\n",
    "    out = multimodal_poc(prompt)\n",
    "\n",
    "    print(\"Done. Results:\", out)\n",
    "\n",
    "    display(Image(filename=out[\"graph\"]))\n",
    "    if \"image\" in out[\"outputs\"]:\n",
    "        display(Image(filename=out[\"outputs\"][\"image\"]))\n",
    "    if \"audio\" in out[\"outputs\"]:\n",
    "        display(Audio(filename=out[\"outputs\"][\"audio\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e646f441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
