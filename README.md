# ğŸŒ Multimodal AI POC  

![Python](https://img.shields.io/badge/python-3.10%2B-blue.svg)  
![License](https://img.shields.io/badge/license-MIT-green.svg)  
![HuggingFace](https://img.shields.io/badge/models-HuggingFace-orange.svg)  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)  

This repository contains a **proof-of-concept multimodal AI pipeline** that combines **knowledge graph extraction, text-to-image, and text-to-audio generation** into a single workflow.  

The goal is to demonstrate how **structured reasoning (via knowledge graphs)** can guide **generative AI models** to produce more meaningful and consistent multimodal outputs.  

---

## âœ¨ Features  

### ğŸ§  Entity & Relation Extraction  
- Extracts **structured knowledge** from text using **Llama 3 + LangChain**.  
- Converts natural language into `(head â†’ relation â†’ tail)` triples.  
- Example:  
  - `thunderstorm â†’ occurs over â†’ lighthouse`  
  - `thunderstorm â†’ produces â†’ thunder sound`  

### ğŸ“Š Knowledge Graph Visualization  
- Builds a **knowledge graph** with `networkx`.  
- Visualizes it using `matplotlib`.  
- Provides **explainability** â€” you can see exactly what the AI understood before generation.  

### ğŸ¨ Image Generation  
- Detects visual cues in the graph.  
- Generates images using **Stable Diffusion (sd-turbo)**.  
- Ensures generated visuals are **faithful to the structured description**.  

### ğŸ”Š Audio Generation  
- Detects sound-related cues in the graph.  
- Uses **MusicGen (facebook/musicgen-small)** to generate **environmental sounds or music**.  
- Example: A thunderstorm produces both a **visual storm scene** and the **sound of thunder**.  

---

## ğŸ›ï¸ Architecture  

The pipeline is modular, with each stage handling a distinct task:  

1. **Input Module** â€“ Accepts natural language user prompts.  
2. **Reasoning Module** â€“ Extracts triples via Llama 3 + LangChain.  
3. **Knowledge Graph Module** â€“ Builds and visualizes the graph.  
4. **Decision Module** â€“ Decides between image and/or audio generation.  
5. **Image Generation Module** â€“ Stable Diffusion creates visuals.  
6. **Audio Generation Module** â€“ MusicGen generates sounds.  
7. **Output Module** â€“ Packages graph, image, and audio into results.  

---

## ğŸ”„ Data Flow  

The system processes information step by step:  

1. **User Prompt** â€“ e.g.  
   *â€œShow me a dramatic thunderstorm over a lighthouse and also provide how thunder sounds.â€*  

2. **Entity & Relation Extraction** â€“ Extract triples:  
   - `thunderstorm â†’ occurs over â†’ lighthouse`  
   - `thunderstorm â†’ produces â†’ thunder sound`  

3. **Knowledge Graph Construction** â€“ Build and visualize graph with entities + relations.  

4. **Modality Decision** â€“ Detect if image or audio generation is needed.  

5. **Image Generation** â€“ Stable Diffusion renders a storm scene over a lighthouse.  

6. **Audio Generation** â€“ MusicGen produces a thunder sound clip.  

7. **Final Output** â€“ You get:  
   - Knowledge graph visualization.  
   - Generated image.  
   - Generated audio.  

---

## ğŸ–¼ Example  

**Prompt:**  
```text
Show me a dramatic thunderstorm over a lighthouse and also provide how thunder sounds.
